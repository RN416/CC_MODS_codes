{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhounan/Documents/Github_Project/CC_MODS_codes\n"
     ]
    }
   ],
   "source": [
    "# system setting\n",
    "import os\n",
    "import sys\n",
    "file_path = globals()['_dh'][0]\n",
    "root_dir = os.path.abspath(os.path.join(file_path, ''))\n",
    "sys.path.append(root_dir)\n",
    "sys.dont_write_bytecode = True\n",
    "os.chdir(root_dir)\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import arange\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from numpy import argmax\n",
    "from sklearn import metrics\n",
    "from imblearn.metrics import sensitivity_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import scipy.stats as ss\n",
    "import itertools\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "import numpy as np, scipy.stats as st\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy import interp\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import AllKNN\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import     OneSidedSelection \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "# import pymrmr\n",
    "import itertools\n",
    "from imblearn.metrics import sensitivity_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the following files are in the current working directory\n",
    "gene_only=pd.read_table('DGE_RMA_MODS_day3day7.txt')\n",
    "\n",
    "# TODO 缺少 DGE_RMA_MODS_day3day7_clinical.txt 文件\n",
    "clinical_only = pd.DataFrame(columns=['Propensity'])\n",
    "clinical_only['Propensity'] = np.random.randint(0, 2, gene_only.shape[0])\n",
    "# clinical_only=pd.read_table('DGE_RMA_MODS_day3day7_clinical.txt')\n",
    "clinical_cols=clinical_only['Propensity']\n",
    "labs= gene_only['Label']\n",
    "labs=labs.astype('category').cat.codes\n",
    "gene_only=gene_only.drop('Label',axis=1)\n",
    "combined = pd.concat([gene_only.reset_index(drop=True), clinical_cols], axis=1)\n",
    "deg=pd.read_table('DEG_day3day7.txt')\n",
    "deg1=deg['X'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction_using_RFECV(model, X, y):\n",
    "    '''\n",
    "    这个函数使用递归特征消除交叉验证（RFECV）方法进行特征选择。\n",
    "    \n",
    "    参数：\n",
    "    model - 用于特征选择的模型\n",
    "    X - 特征数据\n",
    "    y - 标签数据\n",
    "    \n",
    "    返回：\n",
    "    new_X - 特征选择后的数据\n",
    "    '''\n",
    "    #################################################################\n",
    "    # 设置随机种子以确保结果的可复现性\n",
    "    np.random.seed(315)\n",
    "    \n",
    "    # 创建RFECV对象，指定模型、每次迭代中要移除的特征数、交叉验证折数以及评分标准\n",
    "    rfecv = RFECV(estimator=model, step=1, cv=3, scoring='roc_auc', n_jobs=110)\n",
    "    \n",
    "    # 对输入的数据集X和标签y应用RFECV，进行特征选择\n",
    "    rfecv.fit(X, y)\n",
    "    \n",
    "    # 从X中选择RFECV认为重要的特征（即那些被支持的特征）\n",
    "    new_X = X[X.columns[rfecv.support_ == True]]\n",
    "    \n",
    "    # 返回经过特征选择后的数据集\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels(probs, thresh):\n",
    "    return (probs >= thresh).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BRF(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    这个函数使用平衡子样本的随机森林分类器进行特征选择和模型训练。\n",
    "    \n",
    "    参数：\n",
    "    X_train - 训练集的特征\n",
    "    y_train - 训练集的标签\n",
    "    X_test - 测试集的特征\n",
    "    y_test - 测试集的标签\n",
    "    \n",
    "    返回：\n",
    "    sen - 敏感性\n",
    "    spe - 特异性\n",
    "    auc_score - AUC分数\n",
    "    X_reduced.columns - 选择的特征\n",
    "    acc - 准确率\n",
    "    mcc - Matthews相关系数\n",
    "    fp_rate - 假阳性率\n",
    "    tp_rate - 真阳性率\n",
    "    thresholds - 阈值\n",
    "    '''\n",
    "    #################################################################\n",
    "    # 创建一个平衡子样本的随机森林分类器\n",
    "    brf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "    \n",
    "    # 使用随机森林和递归特征消除（RFECV）方法进行特征选择\n",
    "    X_reduced = feature_reduction_using_RFECV(brf, X_train, y_train)\n",
    "    \n",
    "    # 定义网格搜索的参数范围\n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 200, 300, 400, 600, 800, 1000],  # 树的数量\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],  # 最大特征数\n",
    "        'max_depth': [2, 3, 5, 7, 9, 11],  # 树的最大深度\n",
    "    }\n",
    "    \n",
    "    # 使用网格搜索进行参数优化，交叉验证折数为3，评分标准为ROC曲线下面积（AUC）\n",
    "    clf_brf = GridSearchCV(brf, param_grid, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    \n",
    "    # 训练模型并找到最优模型\n",
    "    best_model_brf = clf_brf.fit(X_reduced, y_train)\n",
    "    \n",
    "    # 使用最优模型对测试集进行预测，得到预测概率\n",
    "    y_probs = best_model_brf.predict_proba(X_test[X_reduced.columns])[:, 1]\n",
    "    \n",
    "    # 生成不同的阈值并计算相应的AUC分数\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    scores = [roc_auc_score(y_test, convert_to_labels(y_probs, t)) for t in thresholds]\n",
    "    \n",
    "    # 找出最佳阈值\n",
    "    ix = np.argmax(scores)\n",
    "    \n",
    "    # 根据最佳阈值进行二分类预测\n",
    "    y_test_predictions = np.where(y_probs > thresholds[ix], 1, 0)\n",
    "    \n",
    "    # 计算ROC曲线\n",
    "    fp_rate, tp_rate, thresholds = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # 计算最终的AUC分数\n",
    "    auc_score = roc_auc_score(y_test, y_test_predictions)\n",
    "    \n",
    "    # 计算敏感性和特异性\n",
    "    sen = sensitivity_score(y_test, y_test_predictions, pos_label=0)\n",
    "    spe = specificity_score(y_test, y_test_predictions, pos_label=0)\n",
    "    \n",
    "    # 计算准确率和Matthews相关系数\n",
    "    acc = accuracy_score(y_test, y_test_predictions)\n",
    "    mcc = matthews_corrcoef(y_test, y_test_predictions)\n",
    "    \n",
    "    # 返回所有评价指标和模型信息\n",
    "    return sen, spe, auc_score, X_reduced.columns, acc, mcc, fp_rate, tp_rate, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOGIT(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    这是一个逻辑回归模型的函数，用于训练和测试逻辑回归模型，并返回评价指标和模型信息\n",
    "    \n",
    "    参数：\n",
    "    X_train -- 训练集的特征数据\n",
    "    y_train -- 训练集的标签数据\n",
    "    X_test -- 测试集的特征数据\n",
    "    y_test -- 测试集的标签数据\n",
    "    \n",
    "    返回：\n",
    "    sen -- 敏感性\n",
    "    spe -- 特异性\n",
    "    auc_score -- ROC曲线下的面积\n",
    "    X_reduced.columns -- 特征选择后的特征\n",
    "    acc -- 准确率\n",
    "    mcc -- Matthews相关系数\n",
    "    fp_rate -- 假阳性率\n",
    "    tp_rate -- 真阳性率\n",
    "    thresholds -- 阈值    \n",
    "    '''\n",
    "    #################################################################\n",
    "    # 初始化一个平衡类权重的逻辑回归模型\n",
    "    logit = LogisticRegression(class_weight=\"balanced\")\n",
    "    \n",
    "    # 使用逻辑回归和递归特征消除（RFECV）方法进行特征选择\n",
    "    X_reduced = feature_reduction_using_RFECV(logit, X_train, y_train)\n",
    "    \n",
    "    # 设置正则化类型的候选项\n",
    "    penalty = ['l1', 'l2']\n",
    "    \n",
    "    # 创建正则化强度的超参数空间，范围广泛\n",
    "    C = np.logspace(-3, 3, 10)\n",
    "    \n",
    "    # 创建超参数字典\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    \n",
    "    # 使用网格搜索进行参数优化，交叉验证折数为3，评分标准为ROC曲线下面积（AUC），并指定n_jobs加快计算速度\n",
    "    clf_logit = GridSearchCV(logit, hyperparameters, cv=3, scoring=\"roc_auc\", n_jobs=110)\n",
    "    \n",
    "    # 训练模型并找到最优模型\n",
    "    best_model_logit = clf_logit.fit(X_reduced, y_train)\n",
    "    \n",
    "    # 使用最优模型对测试集进行预测，得到预测概率\n",
    "    y_probs = best_model_logit.predict_proba(X_test[X_reduced.columns])[:, 1]\n",
    "    \n",
    "    # 计算ROC曲线\n",
    "    fp_rate, tp_rate, thresholds = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # 计算AUC分数\n",
    "    auc_score = metrics.auc(fp_rate, tp_rate)\n",
    "    \n",
    "    # 用最佳模型进行预测，得到测试集的预测结果\n",
    "    y_test_predictions = best_model_logit.predict(X_test[X_reduced.columns])\n",
    "    \n",
    "    # 计算敏感性和特异性\n",
    "    sen = sensitivity_score(y_test, y_test_predictions, pos_label=0, average='binary')\n",
    "    spe = specificity_score(y_test, y_test_predictions, pos_label=0, average='binary')\n",
    "    \n",
    "    # 计算准确率和Matthews相关系数\n",
    "    acc = accuracy_score(y_test, y_test_predictions)\n",
    "    mcc = matthews_corrcoef(y_test, y_test_predictions)\n",
    "    \n",
    "    return sen, spe, auc_score, X_reduced.columns, acc, mcc, fp_rate, tp_rate, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    这个函数使用平衡子样本的额外树分类器进行特征选择和模型训练。\n",
    "    \n",
    "    参数：\n",
    "    X_train - 训练集的特征\n",
    "    y_train - 训练集的标签\n",
    "    X_test - 测试集的特征\n",
    "    y_test - 测试集的标签\n",
    "    \n",
    "    返回：\n",
    "    sen - 敏感性\n",
    "    spe - 特异性\n",
    "    auc_score - AUC分数\n",
    "    X_reduced.columns - 选择的特征\n",
    "    acc - 准确率\n",
    "    mcc - Matthews相关系数\n",
    "    fp_rate - 假阳性率\n",
    "    tp_rate - 真阳性率\n",
    "    thresholds - 阈值\n",
    "    '''\n",
    "    #################################################################\n",
    "    # 初始化一个使用平衡子样本的额外树分类器\n",
    "    dt = ExtraTreesClassifier(class_weight=\"balanced_subsample\")\n",
    "    \n",
    "    # 使用递归特征消除（RFECV）和额外树进行特征选择\n",
    "    X_reduced = feature_reduction_using_RFECV(dt, X_train, y_train)\n",
    "    \n",
    "    # 定义额外树分类器的参数范围\n",
    "    parameters = {\n",
    "        'n_estimators': [100, 200, 300, 400, 600, 800, 1000, 1200],  # 决策树的数量\n",
    "        'min_samples_split': [2, 3, 4, 5, 6, 7],  # 分裂内部节点所需的最小样本数\n",
    "        'max_depth': [2, 3, 5, 7],  # 最大树深度\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # 在寻找最佳分割时要考虑的特征数量\n",
    "    }\n",
    "    \n",
    "    # 使用网格搜索调整模型参数，交叉验证折数为3，评分标准为ROC曲线下面积（AUC），使用多核加速\n",
    "    clf_dt = GridSearchCV(dt, parameters, cv=3, scoring=\"roc_auc\", n_jobs=110)\n",
    "    \n",
    "    # 训练模型并找到最优参数的模型\n",
    "    best_model_dt = clf_dt.fit(X_reduced, y_train)\n",
    "    \n",
    "    # 使用最佳模型对测试集进行预测，得到预测概率\n",
    "    y_probs = best_model_dt.predict_proba(X_test[X_reduced.columns])[:, 1]\n",
    "    \n",
    "    # 生成不同的阈值并计算相应的AUC分数\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    scores = [roc_auc_score(y_test, convert_to_labels(y_probs, t)) for t in thresholds]\n",
    "    \n",
    "    # 找出最佳阈值\n",
    "    ix = np.argmax(scores)\n",
    "    \n",
    "    # 根据最佳阈值进行二分类预测\n",
    "    y_test_predictions = np.where(y_probs > thresholds[ix], 1, 0)\n",
    "    \n",
    "    # 计算ROC曲线\n",
    "    fp_rate, tp_rate, thresholds = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # 计算最终的AUC分数\n",
    "    auc_score = roc_auc_score(y_test, y_test_predictions)\n",
    "    \n",
    "    # 计算敏感性和特异性\n",
    "    sen = sensitivity_score(y_test, y_test_predictions, pos_label=0)\n",
    "    spe = specificity_score(y_test, y_test_predictions, pos_label=0)\n",
    "    \n",
    "    # 计算准确率和Matthews相关系数\n",
    "    acc = accuracy_score(y_test, y_test_predictions)\n",
    "    mcc = matthews_corrcoef(y_test, y_test_predictions)\n",
    "    \n",
    "    # 返回所有评价指标和模型信息\n",
    "    return sen, spe, auc_score, X_reduced.columns, acc, mcc, fp_rate, tp_rate, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    '''\n",
    "    计算数据的均值和置信区间。\n",
    "    \n",
    "    参数：\n",
    "    data - 输入数据\n",
    "    confidence - 置信水平\n",
    "    \n",
    "    返回：\n",
    "    m - 数据的均值\n",
    "    m-h - 置信区间下界\n",
    "    m+h - 置信区间上界\n",
    "    '''\n",
    "    #################################################################\n",
    "    # 将输入数据转换为NumPy数组，并确保数据类型为浮点数，以便进行数学运算\n",
    "    a = 1.0 * np.array(data)\n",
    "    \n",
    "    # 计算数据的数量\n",
    "    n = len(a)\n",
    "    \n",
    "    # 计算数据的均值和标准误差（Standard Error of the Mean, SEM）\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    \n",
    "    # 计算置信区间的边界值\n",
    "    # scipy.stats.t.ppf 是t分布的累积分布函数的逆函数，用于根据给定的置信水平计算t值\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    \n",
    "    # 返回数据均值、下界和上界\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_and_validate(X, y, n, sampling, model, deg1):\n",
    "    '''\n",
    "    这个函数用于训练和验证指定的模型，并返回性能指标。\n",
    "    \n",
    "    参数：\n",
    "    X - 特征数据\n",
    "    y - 标签数据\n",
    "    n - 重复次数\n",
    "    sampling - 重采样方法\n",
    "    model - 指定的模型\n",
    "    deg1 - DEG特征列表\n",
    "    \n",
    "    返回：\n",
    "    auc - AUC分数\n",
    "    sensitivity - 敏感性\n",
    "    specificity - 特异性\n",
    "    accuracy - 准确率\n",
    "    m - Matthews相关系数\n",
    "    cols - 特征列表\n",
    "    '''\n",
    "    #################################################################\n",
    "    # 初始化空列表以存储每次迭代的真阳性率\n",
    "    tprs = []\n",
    "    base_fpr = np.linspace(0, 1, 101)  # 定义基准的假阳性率，用于插值和绘图\n",
    "\n",
    "    plt.figure(figsize=(5, 5))  # 初始化图形\n",
    "\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=n)  # 初始化重复的分层交叉验证\n",
    "    i = 0\n",
    "    # 初始化存储各种评估指标的列表\n",
    "    auc = []; sensitivity = []; specificity = []; m = []; accuracy = []; cols = []\n",
    "    \n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        i += 1\n",
    "        print(i, end=\"\")  # 打印当前迭代次数\n",
    "\n",
    "        # 训练集和测试集的划分\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # 使用额外树分类器初始化和训练\n",
    "        forest = ExtraTreesClassifier(n_estimators=500, random_state=42)\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "        forest.fit(X_train, y_train)\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        rf_list = X_train.columns[indices][0:50]  # 取重要性最高的前50个特征\n",
    "        \n",
    "        # 使用Lasso回归进行特征选择\n",
    "        reg = LassoCV(random_state=42, n_jobs=110)\n",
    "        reg.fit(X_train, y_train)\n",
    "        coef = pd.Series(reg.coef_, index=X_train.columns)\n",
    "        selected_f = X_train.columns[coef != 0]  # 选择系数非零的特征\n",
    "        \n",
    "        # 使用mRMR方法进行特征选择\n",
    "        df = X_train\n",
    "        df.insert(loc=0, column='label', value=y_train)\n",
    "        df['label'] = df['label'].astype('category').cat.codes\n",
    "        mrmr = pymrmr.mRMR(df, 'MIQ', 20)  # 选择20个最优特征\n",
    "        \n",
    "        # 合并各方法选出的特征\n",
    "        merged_list = list(itertools.chain(*itertools.zip_longest(rf_list, selected_f, mrmr, deg1)))\n",
    "        merged_list = [i for i in merged_list if i is not None]\n",
    "        \n",
    "        X_train_sel = X_train[set(merged_list)]  # 最终选中的特征\n",
    "        \n",
    "        # 重采样处理不平衡数据\n",
    "        X_samp, y_samp = sampling.fit_resample(X_train_sel, y_train.ravel())\n",
    "        X_samp = pd.DataFrame(X_samp, columns=X_train_sel.columns)\n",
    "        \n",
    "        # 根据指定模型进行训练和评估\n",
    "        if model == \"BRF\":\n",
    "            sen, spe, auc_score, feat, acc, mcc, fpr, tpr, thresholds = BRF(X_samp, y_samp, X_test, y_test)\n",
    "        # 其他模型的调用类似\n",
    "        \n",
    "        plt.plot(fpr, tpr, 'b', alpha=0.05)  # 绘制ROC曲线\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "        # 存储各种评估指标\n",
    "        auc.append(auc_score)\n",
    "        sensitivity.append(sen)\n",
    "        specificity.append(spe)\n",
    "        accuracy.append(acc)\n",
    "        m.append(mcc)\n",
    "        cols.append(feat)\n",
    "\n",
    "    # 计算平均和标准差，用于绘制平均ROC曲线和置信区间\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "    \n",
    "    # 结果保存和绘图\n",
    "    plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "    plt.savefig('roc.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # 其他结果处理和保存\n",
    "    return auc, sensitivity, specificity, accuracy, m, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=ADASYN(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ADASYN(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ADASYN</label><div class=\"sk-toggleable__content\"><pre>ADASYN(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ADASYN(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DONOT RUN Takes a very long time\n",
    "cc=ADASYN(random_state=42)\n",
    "auc, sensitivity, specificity,accuracy,m,cols=model_fit_and_validate(combined, labs, 7, cc, \"BRF\",deg1)\n",
    "l=[np.mean(auc),mean_confidence_interval(auc)[1],mean_confidence_interval(auc)[2],np.mean(sensitivity),mean_confidence_interval(sensitivity)[1],mean_confidence_interval(sensitivity)[2],np.mean(specificity),mean_confidence_interval(specificity)[1],mean_confidence_interval(specificity)[2],np.mean(m),mean_confidence_interval(m)[1],mean_confidence_interval(m)[2]]\n",
    "row=pd.Series(l,['auc','auc_lci','auc_rci','sen','sen_lci','sen_rci','spe','spe_lci','spe_rci','m','m_lci','m_rci'])\n",
    "df=pd.DataFrame()\n",
    "df=df.append([row],ignore_index=True)\n",
    "#df.to_csv('/data/shayantan/Normalized_Data_for_training/GSE152075_results/GSE152075_CC_LOGIT_30.csv',index=False)\n",
    "flat_list = [item for sublist in cols for item in sublist]\n",
    "x=Counter(flat_list)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MODS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
